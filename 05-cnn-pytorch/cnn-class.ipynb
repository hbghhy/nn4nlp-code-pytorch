{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T08:49:10.825856Z",
     "start_time": "2018-04-14T08:49:10.622317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20285edc410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T08:49:11.148714Z",
     "start_time": "2018-04-14T08:49:10.827888Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EMB_SIZE = 64\n",
    "WIN_SIZE = 3\n",
    "FILTER_SIZE = 64\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            tag, words = line.lower().strip().split(\" ||| \")\n",
    "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/classes/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/classes/test.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T08:49:11.168782Z",
     "start_time": "2018-04-14T08:49:11.150720Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, nwords,ntags, emb_size,filter_size,win_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(nwords, emb_size,padding_idx=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(emb_size, filter_size,win_size,padding=1),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.dropout=nn.Dropout(p=0.75)\n",
    "        self.linear=nn.Linear(emb_size,ntags)\n",
    "\n",
    "    def forward(self, words):\n",
    "        #print(words)\n",
    "        emb = self.embedding(words)    \n",
    "        feat = self.conv(emb.permute((0,2,1))) \n",
    "        logit = self.linear(self.dropout(feat.max(dim=2)[0]))        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T08:49:13.598369Z",
     "start_time": "2018-04-14T08:49:11.171777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model and the optimizer\n",
    "model = CNN(nwords=nwords,ntags=ntags, emb_size=EMB_SIZE,filter_size=FILTER_SIZE,win_size=WIN_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T08:49:13.650459Z",
     "start_time": "2018-04-14T08:49:13.599321Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert a (nested) list of int into a pytorch Variable\n",
    "def convert_to_variable(words):\n",
    "    var = Variable(torch.LongTensor(words))\n",
    "    if USE_CUDA:\n",
    "        var = var.cuda()\n",
    "    return var\n",
    "\n",
    "# A function to calculate scores for one value\n",
    "def calc_scores(words):\n",
    "    # This will change from a list of histories, to a pytorch Variable whose data type is LongTensor\n",
    "    #print(words)\n",
    "    words_var = convert_to_variable(words)\n",
    "    logits = model(words_var)\n",
    "    return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T08:54:46.728956Z",
     "start_time": "2018-04-14T08:49:13.655477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=1.6738, acc=0.2496, time=32.70s\n",
      "iter 0: test acc=0.2575\n",
      "iter 1: train loss/sent=1.6254, acc=0.2570, time=31.69s\n",
      "iter 1: test acc=0.2995\n",
      "iter 2: train loss/sent=1.6020, acc=0.2788, time=31.42s\n",
      "iter 2: test acc=0.2900\n",
      "iter 3: train loss/sent=1.5690, acc=0.3017, time=31.60s\n",
      "iter 3: test acc=0.3303\n",
      "iter 4: train loss/sent=1.5110, acc=0.3428, time=31.68s\n",
      "iter 4: test acc=0.3294\n",
      "iter 5: train loss/sent=1.4496, acc=0.3603, time=31.51s\n",
      "iter 5: test acc=0.3471\n",
      "iter 6: train loss/sent=1.3747, acc=0.4017, time=32.08s\n",
      "iter 6: test acc=0.3548\n",
      "iter 7: train loss/sent=1.3206, acc=0.4244, time=31.84s\n",
      "iter 7: test acc=0.3778\n",
      "iter 8: train loss/sent=1.2655, acc=0.4569, time=32.15s\n",
      "iter 8: test acc=0.3710\n",
      "iter 9: train loss/sent=1.2096, acc=0.4714, time=32.29s\n",
      "iter 9: test acc=0.3796\n"
     ]
    }
   ],
   "source": [
    "for ITER in range(10):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0.0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for words, tag in train:\n",
    "        if len(words) < WIN_SIZE:\n",
    "            words += [0] * (WIN_SIZE-len(words))\n",
    "        scores = calc_scores([words])\n",
    "        predict = scores.max(1)[1].data[0]\n",
    "        if predict == tag:\n",
    "            train_correct += 1\n",
    "\n",
    "        my_loss = nn.functional.cross_entropy(scores, convert_to_variable([tag]))\n",
    "        train_loss += my_loss.data[0]\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/sent=%.4f, acc=%.4f, time=%.2fs\" % (ITER, train_loss/len(train), train_correct/len(train), time.time()-start))\n",
    "    # Perform testing\n",
    "    model.eval()\n",
    "    test_correct = 0.0\n",
    "    for words, tag in dev:\n",
    "        scores = calc_scores([words])\n",
    "        predict = scores.max(1)[1].data[0]\n",
    "        if predict == tag:\n",
    "            test_correct += 1\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, test_correct/len(dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
