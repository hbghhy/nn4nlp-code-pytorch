{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T16:23:53.466551Z",
     "start_time": "2018-04-13T16:23:51.911732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e97e7dd5b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T16:23:54.786145Z",
     "start_time": "2018-04-13T16:23:53.468559Z"
    }
   },
   "outputs": [],
   "source": [
    "K=3 #number of negative samples\n",
    "N = 2 # The length of the n-gram\n",
    "EMB_SIZE = 64 # The size of the embedding\n",
    "BATCH_SIZE=512\n",
    "\n",
    "embeddings_location = \"embeddings.txt\" #the file to write the word embeddings to\n",
    "labels_location = \"labels.txt\" #the file to write the labels to\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# Functions to read in the corpus\n",
    "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
    "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
    "#       data we would have to do pre-processing and consider how to choose\n",
    "#       unknown words, etc.\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "#word counts for negative sampling\n",
    "word_counts = defaultdict(int)\n",
    "\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            for word in line:\n",
    "                word_counts[w2i[word]] += 1\n",
    "            yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i)\n",
    "\n",
    "\n",
    "# take the word counts to the 3/4, normalize\n",
    "counts =  np.array([list(x) for x in word_counts.items()])[:,1]**.75\n",
    "normalizing_constant = sum(counts)\n",
    "word_probabilities = np.zeros(nwords)\n",
    "for word_id in word_counts:\n",
    "    word_probabilities[word_id] = word_counts[word_id]**.75/normalizing_constant\n",
    "\n",
    "\n",
    "with open(labels_location, 'w') as labels_file:\n",
    "    for i in range(nwords):\n",
    "        labels_file.write(i2w[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T16:23:54.797093Z",
     "start_time": "2018-04-13T16:23:54.788073Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SKIP_NS(nn.Module):\n",
    "    def __init__(self, nwords, emb_size):\n",
    "        super(SKIP_NS, self).__init__()\n",
    "        self.embedding = nn.Embedding(nwords, emb_size)\n",
    "\n",
    "    def forward(self, words):\n",
    "        #print(words)\n",
    "        emb=self.embedding(words)    # 3D Tensor of size [batch_size x 2x emb_size]\n",
    "        #print(emb1*emb2)\n",
    "        return (emb[:,0,:]*emb[:,1,:]).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T16:23:58.207977Z",
     "start_time": "2018-04-13T16:23:54.799094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model and the optimizer\n",
    "model = SKIP_NS(nwords=nwords, emb_size=EMB_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T16:24:18.863913Z",
     "start_time": "2018-04-13T16:23:58.209984Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert a (nested) list of int into a pytorch tensor\n",
    "def convert_to_tensor(words):\n",
    "    var = torch.LongTensor(words)\n",
    "#     if USE_CUDA:\n",
    "#         var = var.cuda()\n",
    "    return var\n",
    "\n",
    "# convert a (nested) list of int into a pytorch Variable\n",
    "def convert_to_variable(tensor):\n",
    "    var = Variable(tensor)\n",
    "    if USE_CUDA:\n",
    "        var = var.cuda()\n",
    "\n",
    "    return var\n",
    "\n",
    "def convert_to_loader(train):\n",
    "    \n",
    "    all_histories = []\n",
    "    all_targets = []\n",
    "    for sent in train:\n",
    "        if len(sent)==0:\n",
    "            continue\n",
    "        all_neg_words = np.random.choice(nwords, size=2*N*K*len(sent), replace=True, p=word_probabilities).tolist()\n",
    "        # The initial history is equal to end of sentence symbols\n",
    "        padded_sent = [S] * N + sent + [S] * N\n",
    "        # Step through the sentence, including the end of sentence token\n",
    "\n",
    "        for i in range(0,len(sent)):\n",
    "            neg_words = all_neg_words[i*K*2*N:(i+1)*K*2*N]\n",
    "            pos_words = ([sent[x] if x >= 0 else S for x in range(i-N,i)] +\n",
    "                     [sent[x] if x < len(sent) else S for x in range(i+1,i+N+1)])\n",
    "            for pos_word in pos_words:\n",
    "                all_histories.append([sent[i],pos_word])\n",
    "                all_targets.append(1)\n",
    "            for neg_word in neg_words:\n",
    "                all_histories.append([sent[i],neg_word])         \n",
    "                all_targets.append(0)\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset = TensorDataset(data_tensor=convert_to_tensor(all_histories), \n",
    "                                       target_tensor=convert_to_tensor(all_targets))\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               \n",
    "        num_workers=0\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "train_loader=convert_to_loader(train)\n",
    "dev_loader=convert_to_loader(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T16:36:15.363411Z",
     "start_time": "2018-04-13T16:24:18.865892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/word=0.1092, ppl=1.1153 (word/sec=122282.94)\n",
      "saving embedding files\n",
      "iter 0: dev loss/word=0.0820, ppl=1.0854 (word/sec=126529.83)\n",
      "iter 1: train loss/word=0.0685, ppl=1.0709 (word/sec=105992.17)\n",
      "saving embedding files\n",
      "iter 1: dev loss/word=0.0746, ppl=1.0774 (word/sec=141617.11)\n",
      "iter 2: train loss/word=0.0657, ppl=1.0679 (word/sec=101518.63)\n",
      "saving embedding files\n",
      "iter 2: dev loss/word=0.0738, ppl=1.0766 (word/sec=150767.95)\n",
      "iter 3: train loss/word=0.0642, ppl=1.0663 (word/sec=103025.02)\n",
      "saving embedding files\n",
      "iter 3: dev loss/word=0.0711, ppl=1.0737 (word/sec=106952.10)\n",
      "iter 4: train loss/word=0.0635, ppl=1.0656 (word/sec=98716.45)\n",
      "lr decay\n",
      "iter 4: dev loss/word=0.0774, ppl=1.0804 (word/sec=113988.57)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "last_dev = 1e20\n",
    "best_dev = 1e20\n",
    "\n",
    "for ITER in range(5):\n",
    "    # Perform training\n",
    "    model.train()\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        \n",
    "        batch_x=convert_to_variable(batch_x)\n",
    "        batch_y=convert_to_variable(batch_y.float())\n",
    "        #print(sent)\n",
    "        predictions=model(batch_x)\n",
    "        my_loss = nn.functional.binary_cross_entropy_with_logits(predictions, batch_y,size_average=False)\n",
    "        train_loss += my_loss.data[0]\n",
    "        train_words += len(batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), train_words/(time.time()-start)))\n",
    "\n",
    "    # Evaluate on dev set\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for step,(batch_x, batch_y) in enumerate(dev_loader):\n",
    "        batch_x=convert_to_variable(batch_x)\n",
    "        batch_y=convert_to_variable(batch_y.float())\n",
    "        predictions=model(batch_x)\n",
    "        my_loss = nn.functional.binary_cross_entropy_with_logits(predictions, batch_y,size_average=False)\n",
    "        dev_loss += my_loss.data[0]\n",
    "        dev_words += len(batch_y)\n",
    "\n",
    "    # Keep track of the development accuracy and reduce the learning rate if it got worse\n",
    "    if last_dev < dev_loss:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr']/=2\n",
    "        print('lr decay')\n",
    "    last_dev = dev_loss\n",
    "\n",
    "    # Keep track of the best development accuracy, and save the model only if it's the best one\n",
    "    if best_dev > dev_loss:\n",
    "        print(\"saving embedding files\")\n",
    "        with open(embeddings_location, 'w') as embeddings_file:\n",
    "            W_w_np = model.embedding.weight.cpu().data.numpy()\n",
    "            for i in range(nwords):\n",
    "                ith_embedding = '\\t'.join(map(str, W_w_np[i]))\n",
    "                embeddings_file.write(ith_embedding + '\\n')\n",
    "        best_dev = dev_loss\n",
    "        \n",
    "        # Save the model\n",
    "    print(\"iter %r: dev loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), dev_words/(time.time()-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
