{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T14:29:11.192938Z",
     "start_time": "2018-04-13T14:29:10.971321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fce098c350>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "torch.manual_seed(1)    # reproducible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T14:29:11.719301Z",
     "start_time": "2018-04-13T14:29:11.194908Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "N = 2 # The length of the n-gram\n",
    "EMB_SIZE = 128 # The size of the embedding\n",
    "\n",
    "BATCH_SIZE=64\n",
    "\n",
    "embeddings_location = \"embeddings.txt\" #the file to write the word embeddings to\n",
    "labels_location = \"labels.txt\" #the file to write the labels to\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# Functions to read in the corpus\n",
    "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
    "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
    "#       data we would have to do pre-processing and consider how to choose\n",
    "#       unknown words, etc.\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i)\n",
    "\n",
    "\n",
    "with open(labels_location, 'w') as labels_file:\n",
    "    for i in range(nwords):\n",
    "        labels_file.write(i2w[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T14:29:11.732338Z",
     "start_time": "2018-04-13T14:29:11.722336Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, nwords, emb_size, num_context):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(nwords, emb_size)\n",
    "        self.linear = nn.Linear(emb_size, nwords)\n",
    "\n",
    "    def forward(self, words):\n",
    "        #print(words)\n",
    "        feat = self.embedding(words).sum(1)      # 2D Tensor of size [batch_size x  emb_size]\n",
    "        logit = self.linear(feat)           # 2D Tensor of size [batch_size x nwords]\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T14:29:14.597820Z",
     "start_time": "2018-04-13T14:29:11.737352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model and the optimizer\n",
    "model = CBOW(nwords=nwords, emb_size=EMB_SIZE,num_context=N)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T14:29:16.389094Z",
     "start_time": "2018-04-13T14:29:14.599322Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert a (nested) list of int into a pytorch tensor\n",
    "def convert_to_tensor(words):\n",
    "    var = torch.LongTensor(words)\n",
    "#     if USE_CUDA:\n",
    "#         var = var.cuda()\n",
    "    return var\n",
    "\n",
    "# convert a (nested) list of int into a pytorch Variable\n",
    "def convert_to_variable(tensor):\n",
    "    var = Variable(tensor)\n",
    "    if USE_CUDA:\n",
    "        var = var.cuda()\n",
    "\n",
    "    return var\n",
    "\n",
    "def convert_to_loader(train):\n",
    "    all_histories = []\n",
    "    all_targets = []\n",
    "    for sent in train:\n",
    "        if len(sent)==0:\n",
    "            continue\n",
    "        # The initial history is equal to end of sentence symbols\n",
    "        padded_sent = [S] * N + sent + [S] * N\n",
    "        # Step through the sentence, including the end of sentence token\n",
    "\n",
    "        for i in range(N,N+len(sent)):\n",
    "            all_histories.append(list(padded_sent[i-N:i]+padded_sent[i+1:i+N+1]))\n",
    "            all_targets.append(padded_sent[i])\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset = TensorDataset(data_tensor=convert_to_tensor(all_histories), \n",
    "                                       target_tensor=convert_to_tensor(all_targets))\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               \n",
    "        num_workers=0\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "train_loader=convert_to_loader(train)\n",
    "dev_loader=convert_to_loader(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T14:48:07.433543Z",
     "start_time": "2018-04-13T14:29:16.392102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/word=8.3928, ppl=4415.3643 (word/sec=24490.70)\n",
      "saving embedding files\n",
      "iter 0: dev loss/word=7.1321, ppl=1251.4689 (word/sec=29014.93)\n",
      "iter 1: train loss/word=6.4828, ppl=653.7773 (word/sec=20865.78)\n",
      "saving embedding files\n",
      "iter 1: dev loss/word=6.6759, ppl=793.0255 (word/sec=25129.78)\n",
      "iter 2: train loss/word=5.9186, ppl=371.8960 (word/sec=21381.67)\n",
      "saving embedding files\n",
      "iter 2: dev loss/word=6.4141, ppl=610.3669 (word/sec=26296.27)\n",
      "iter 3: train loss/word=5.6017, ppl=270.8930 (word/sec=21153.32)\n",
      "saving embedding files\n",
      "iter 3: dev loss/word=6.3974, ppl=600.2663 (word/sec=24867.28)\n",
      "iter 4: train loss/word=5.3891, ppl=219.0147 (word/sec=20940.79)\n",
      "saving embedding files\n",
      "iter 4: dev loss/word=6.2867, ppl=537.3678 (word/sec=24920.16)\n",
      "iter 5: train loss/word=5.2350, ppl=187.7259 (word/sec=20936.03)\n",
      "saving embedding files\n",
      "iter 5: dev loss/word=6.2610, ppl=523.7229 (word/sec=25189.31)\n",
      "iter 6: train loss/word=5.1154, ppl=166.5752 (word/sec=20524.67)\n",
      "lr decay\n",
      "iter 6: dev loss/word=6.2744, ppl=530.8163 (word/sec=51187.95)\n",
      "iter 7: train loss/word=4.3401, ppl=76.7116 (word/sec=20981.79)\n",
      "saving embedding files\n",
      "iter 7: dev loss/word=5.8050, ppl=331.9711 (word/sec=25644.99)\n",
      "iter 8: train loss/word=4.2359, ppl=69.1235 (word/sec=21055.98)\n",
      "lr decay\n",
      "iter 8: dev loss/word=5.8356, ppl=342.2775 (word/sec=64494.73)\n",
      "iter 9: train loss/word=3.8713, ppl=48.0029 (word/sec=24900.32)\n",
      "saving embedding files\n",
      "iter 9: dev loss/word=5.6768, ppl=292.0277 (word/sec=31452.54)\n",
      "iter 10: train loss/word=3.8221, ppl=45.6993 (word/sec=22500.45)\n",
      "lr decay\n",
      "iter 10: dev loss/word=5.7147, ppl=303.2970 (word/sec=47254.56)\n",
      "iter 11: train loss/word=3.6331, ppl=37.8313 (word/sec=18439.88)\n",
      "saving embedding files\n",
      "iter 11: dev loss/word=5.6419, ppl=282.0078 (word/sec=10758.06)\n",
      "iter 12: train loss/word=3.6082, ppl=36.8985 (word/sec=12683.85)\n",
      "lr decay\n",
      "iter 12: dev loss/word=5.6620, ppl=287.7168 (word/sec=28324.15)\n",
      "iter 13: train loss/word=3.5087, ppl=33.4057 (word/sec=12379.28)\n",
      "saving embedding files\n",
      "iter 13: dev loss/word=5.6374, ppl=280.7452 (word/sec=19244.04)\n",
      "iter 14: train loss/word=3.4965, ppl=32.9995 (word/sec=12111.22)\n",
      "lr decay\n",
      "iter 14: dev loss/word=5.6531, ppl=285.1788 (word/sec=23641.30)\n",
      "iter 15: train loss/word=3.4446, ppl=31.3310 (word/sec=11665.46)\n",
      "iter 15: dev loss/word=5.6421, ppl=282.0593 (word/sec=26521.73)\n",
      "iter 16: train loss/word=3.4384, ppl=31.1378 (word/sec=11828.40)\n",
      "lr decay\n",
      "iter 16: dev loss/word=5.6473, ppl=283.5149 (word/sec=28759.93)\n",
      "iter 17: train loss/word=3.4111, ppl=30.2983 (word/sec=12322.60)\n",
      "iter 17: dev loss/word=5.6429, ppl=282.2891 (word/sec=29232.80)\n",
      "iter 18: train loss/word=3.4080, ppl=30.2056 (word/sec=12715.85)\n",
      "lr decay\n",
      "iter 18: dev loss/word=5.6452, ppl=282.9217 (word/sec=27852.77)\n",
      "iter 19: train loss/word=3.3936, ppl=29.7739 (word/sec=12407.94)\n",
      "iter 19: dev loss/word=5.6446, ppl=282.7672 (word/sec=27126.21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "last_dev = 1e20\n",
    "best_dev = 1e20\n",
    "\n",
    "for ITER in range(20):\n",
    "    # Perform training\n",
    "    model.train()\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        \n",
    "        batch_x=convert_to_variable(batch_x)\n",
    "        batch_y=convert_to_variable(batch_y)\n",
    "        #print(sent)\n",
    "        logits=model(batch_x)\n",
    "        my_loss = nn.functional.cross_entropy(logits, batch_y, size_average=False)\n",
    "        train_loss += my_loss.data[0]\n",
    "        train_words += len(batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), train_words/(time.time()-start)))\n",
    "\n",
    "    # Evaluate on dev set\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for step,(batch_x, batch_y) in enumerate(dev_loader):\n",
    "        batch_x=convert_to_variable(batch_x)\n",
    "        batch_y=convert_to_variable(batch_y)\n",
    "        logits=model(batch_x)\n",
    "        my_loss = nn.functional.cross_entropy(logits, batch_y, size_average=False)\n",
    "        dev_loss += my_loss.data[0]\n",
    "        dev_words += len(batch_y)\n",
    "\n",
    "    # Keep track of the development accuracy and reduce the learning rate if it got worse\n",
    "    if last_dev < dev_loss:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr']/=2\n",
    "        print('lr decay')\n",
    "    last_dev = dev_loss\n",
    "\n",
    "    # Keep track of the best development accuracy, and save the model only if it's the best one\n",
    "    if best_dev > dev_loss:\n",
    "        print(\"saving embedding files\")\n",
    "        with open(embeddings_location, 'w') as embeddings_file:\n",
    "            W_w_np = model.embedding.weight.cpu().data.numpy()\n",
    "            for i in range(nwords):\n",
    "                ith_embedding = '\\t'.join(map(str, W_w_np[i]))\n",
    "                embeddings_file.write(ith_embedding + '\\n')\n",
    "        best_dev = dev_loss\n",
    "        \n",
    "        # Save the model\n",
    "    print(\"iter %r: dev loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), dev_words/(time.time()-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
