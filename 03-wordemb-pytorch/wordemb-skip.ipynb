{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T15:29:42.712429Z",
     "start_time": "2018-04-13T15:29:42.445221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20775221310>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T15:29:43.391256Z",
     "start_time": "2018-04-13T15:29:42.714935Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "N = 2 # The length of the n-gram\n",
    "EMB_SIZE = 128 # The size of the embedding\n",
    "\n",
    "embeddings_location = \"embeddings.txt\" #the file to write the word embeddings to\n",
    "labels_location = \"labels.txt\" #the file to write the labels to\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# Functions to read in the corpus\n",
    "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
    "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
    "#       data we would have to do pre-processing and consider how to choose\n",
    "#       unknown words, etc.\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i)\n",
    "\n",
    "\n",
    "with open(labels_location, 'w') as labels_file:\n",
    "    for i in range(nwords):\n",
    "        labels_file.write(i2w[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T15:29:43.408780Z",
     "start_time": "2018-04-13T15:29:43.393740Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SKIP(nn.Module):\n",
    "    def __init__(self, nwords, emb_size):\n",
    "        super(SKIP, self).__init__()\n",
    "        self.embedding = nn.Embedding(nwords, emb_size)\n",
    "        self.linear = nn.Linear(emb_size, nwords)\n",
    "\n",
    "    def forward(self, words):\n",
    "        #print(words)\n",
    "        emb = self.embedding(words)    # 3D Tensor of size [batch_size x 1 x emb_size]\n",
    "        feat = emb.view(emb.size(0), -1) # 2D Tensor of size [batch_size x emb_size]\n",
    "        logit = self.linear(feat)           # 2D Tensor of size [batch_size x nwords]\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T15:29:46.677573Z",
     "start_time": "2018-04-13T15:29:43.411288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model and the optimizer\n",
    "model = SKIP(nwords=nwords, emb_size=EMB_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T15:29:51.199837Z",
     "start_time": "2018-04-13T15:29:46.680051Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert a (nested) list of int into a pytorch tensor\n",
    "def convert_to_tensor(words):\n",
    "    var = torch.LongTensor(words)\n",
    "#     if USE_CUDA:\n",
    "#         var = var.cuda()\n",
    "    return var\n",
    "\n",
    "# convert a (nested) list of int into a pytorch Variable\n",
    "def convert_to_variable(tensor):\n",
    "    var = Variable(tensor)\n",
    "    if USE_CUDA:\n",
    "        var = var.cuda()\n",
    "\n",
    "    return var\n",
    "\n",
    "def convert_to_loader(train):\n",
    "    all_histories = []\n",
    "    all_targets = []\n",
    "    for sent in train:\n",
    "        if len(sent)==0:\n",
    "            continue\n",
    "        for i in range(0,len(sent)):\n",
    "            for j in range(1,N+1):\n",
    "                all_histories.append([sent[i]])\n",
    "                all_histories.append([sent[i]])\n",
    "                all_targets.append(sent[i-j] if i-j >= 0 else S)\n",
    "                all_targets.append(sent[i+j] if i+j< len(sent) else S)\n",
    "\n",
    "    train_dataset = TensorDataset(data_tensor=convert_to_tensor(all_histories), \n",
    "                                       target_tensor=convert_to_tensor(all_targets))\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               \n",
    "        num_workers=0\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "train_loader=convert_to_loader(train)\n",
    "dev_loader=convert_to_loader(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T15:43:31.775858Z",
     "start_time": "2018-04-13T15:29:51.204352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/word=6.6550, ppl=776.6416 (word/sec=21484.70)\n",
      "saving embedding files\n",
      "iter 0: dev loss/word=6.4265, ppl=617.9775 (word/sec=40085.39)\n",
      "iter 1: train loss/word=6.2485, ppl=517.2380 (word/sec=22325.43)\n",
      "saving embedding files\n",
      "iter 1: dev loss/word=6.2913, ppl=539.8292 (word/sec=42527.15)\n",
      "iter 2: train loss/word=6.1147, ppl=452.4713 (word/sec=22567.33)\n",
      "saving embedding files\n",
      "iter 2: dev loss/word=6.2490, ppl=517.5196 (word/sec=48141.58)\n",
      "iter 3: train loss/word=6.0355, ppl=418.0194 (word/sec=24687.85)\n",
      "saving embedding files\n",
      "iter 3: dev loss/word=6.2259, ppl=505.6799 (word/sec=44144.88)\n",
      "iter 4: train loss/word=5.9814, ppl=395.9856 (word/sec=21844.80)\n",
      "saving embedding files\n",
      "iter 4: dev loss/word=6.2139, ppl=499.6622 (word/sec=42013.13)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "last_dev = 1e20\n",
    "best_dev = 1e20\n",
    "\n",
    "for ITER in range(5):\n",
    "    # Perform training\n",
    "    model.train()\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        \n",
    "        batch_x=convert_to_variable(batch_x)\n",
    "        batch_y=convert_to_variable(batch_y)\n",
    "        #print(sent)\n",
    "        logits=model(batch_x)\n",
    "        my_loss = nn.functional.cross_entropy(logits, batch_y, size_average=False)\n",
    "        train_loss += my_loss.data[0]\n",
    "        train_words += len(batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), train_words/(time.time()-start)))\n",
    "\n",
    "    # Evaluate on dev set\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for step,(batch_x, batch_y) in enumerate(dev_loader):\n",
    "        batch_x=convert_to_variable(batch_x)\n",
    "        batch_y=convert_to_variable(batch_y)\n",
    "        logits=model(batch_x)\n",
    "        my_loss = nn.functional.cross_entropy(logits, batch_y, size_average=False)\n",
    "        dev_loss += my_loss.data[0]\n",
    "        dev_words += len(batch_y)\n",
    "\n",
    "    # Keep track of the development accuracy and reduce the learning rate if it got worse\n",
    "    if last_dev < dev_loss:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr']/=2\n",
    "        print('lr decay')\n",
    "    last_dev = dev_loss\n",
    "\n",
    "    # Keep track of the best development accuracy, and save the model only if it's the best one\n",
    "    if best_dev > dev_loss:\n",
    "        print(\"saving embedding files\")\n",
    "        with open(embeddings_location, 'w') as embeddings_file:\n",
    "            W_w_np = model.embedding.weight.cpu().data.numpy()\n",
    "            for i in range(nwords):\n",
    "                ith_embedding = '\\t'.join(map(str, W_w_np[i]))\n",
    "                embeddings_file.write(ith_embedding + '\\n')\n",
    "        best_dev = dev_loss\n",
    "        \n",
    "        # Save the model\n",
    "    print(\"iter %r: dev loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), dev_words/(time.time()-start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
