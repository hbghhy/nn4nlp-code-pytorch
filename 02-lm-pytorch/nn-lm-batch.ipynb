{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T11:42:26.318340Z",
     "start_time": "2018-04-04T11:42:22.829929Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T11:42:26.345413Z",
     "start_time": "2018-04-04T11:42:26.325360Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Feed-forward Neural Network Language Model\n",
    "class FNN_LM(nn.Module):\n",
    "    def __init__(self, nwords, emb_size, hid_size, num_hist, dropout):\n",
    "        super(FNN_LM, self).__init__()\n",
    "        self.embedding = nn.Embedding(nwords, emb_size)\n",
    "        self.fnn = nn.Sequential(\n",
    "          nn.Linear(num_hist*emb_size, hid_size), nn.Tanh(),\n",
    "          nn.Dropout(dropout),\n",
    "          nn.Linear(hid_size, nwords)\n",
    "        )\n",
    "\n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)      # 3D Tensor of size [batch_size x num_hist x emb_size]\n",
    "        feat = emb.view(emb.size(0), -1) # 2D Tensor of size [batch_size x (num_hist*emb_size)]\n",
    "        logit = self.fnn(feat)           # 2D Tensor of size [batch_size x nwords]\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T11:42:27.201187Z",
     "start_time": "2018-04-04T11:42:26.348923Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "N = 2 # The length of the n-gram\n",
    "EMB_SIZE = 128 # The size of the embedding\n",
    "HID_SIZE = 128 # The size of the hidden layer\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# Functions to read in the corpus\n",
    "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
    "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
    "#       data we would have to do pre-processing and consider how to choose\n",
    "#       unknown words, etc.\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T11:42:32.681069Z",
     "start_time": "2018-04-04T11:42:27.207705Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model and the optimizer\n",
    "model = FNN_LM(nwords=nwords, emb_size=EMB_SIZE, hid_size=HID_SIZE, num_hist=N, dropout=0.2)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T11:42:32.716665Z",
     "start_time": "2018-04-04T11:42:32.682574Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# convert a (nested) list of int into a pytorch Variable\n",
    "def convert_to_variable(words):\n",
    "    var = Variable(torch.LongTensor(words))\n",
    "    if USE_CUDA:\n",
    "        var = var.cuda()\n",
    "\n",
    "    return var\n",
    "\n",
    "# A function to calculate scores for one value\n",
    "def calc_score_of_histories(words):\n",
    "    # This will change from a list of histories, to a pytorch Variable whose data type is LongTensor\n",
    "    words_var = convert_to_variable(words)\n",
    "    logits = model(words_var)\n",
    "    return logits\n",
    "\n",
    "# Calculate the loss value for the entire sentence\n",
    "def calc_sent_loss(sent):\n",
    "    # The initial history is equal to end of sentence symbols\n",
    "    hist = [S] * N\n",
    "    # Step through the sentence, including the end of sentence token\n",
    "    all_histories = []\n",
    "    all_targets = []\n",
    "    for next_word in sent + [S]:\n",
    "        all_histories.append(list(hist))\n",
    "        all_targets.append(next_word)\n",
    "        hist = hist[1:] + [next_word]\n",
    "\n",
    "    logits = calc_score_of_histories(all_histories)\n",
    "    loss = nn.functional.cross_entropy(logits, convert_to_variable(all_targets), size_average=False)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T11:42:32.739225Z",
     "start_time": "2018-04-04T11:42:32.720680Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 100\n",
    "# Generate a sentence\n",
    "def generate_sent():\n",
    "    hist = [S] * N\n",
    "    sent = []\n",
    "    while True:\n",
    "        logits = calc_score_of_histories([hist])\n",
    "        prob = nn.functional.softmax(logits)\n",
    "        next_word = prob.multinomial().data[0,0]\n",
    "        if next_word == S or len(sent) == MAX_LEN:\n",
    "            break\n",
    "        sent.append(next_word)\n",
    "        hist = hist[1:] + [next_word]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T12:06:45.658507Z",
     "start_time": "2018-04-04T11:42:32.745245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 5000 sentences (word/sec=3544.34)\n",
      "--finished 10000 sentences (word/sec=3597.20)\n",
      "--finished 15000 sentences (word/sec=3564.25)\n",
      "--finished 20000 sentences (word/sec=3508.55)\n",
      "--finished 25000 sentences (word/sec=3493.26)\n",
      "--finished 30000 sentences (word/sec=3436.04)\n",
      "--finished 35000 sentences (word/sec=3422.55)\n",
      "--finished 40000 sentences (word/sec=3417.21)\n",
      "iter 0: train loss/word=6.2635, ppl=525.0310 (word/sec=3412.49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\envs\\fastai\\lib\\site-packages\\torch\\serialization.py:158: UserWarning: Couldn't retrieve source code for container of type FNN_LM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda2\\envs\\fastai\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: dev loss/word=5.8920, ppl=362.1368 (word/sec=22378.80)\n",
      "mr. profit crash up world\n",
      "posted yields in east germany at N N drop to any about three of N to realize a lighter first stretch on <unk> one of treasury securities fell N N N days in an wrong compiled while the pound was <unk> prices and the when mr. <unk> says if opposition deficits of a $ N\n",
      "the size of $ N a six-month mentioned of her medicare sex fees N N in real estate unit are york-based new jersey\n",
      "the new company reported third-quarter profit of lower steel prices have been line and general democrat the approval of harder their own from the year and cash of analysts was\n",
      "the creditors so younger that messrs. consistent\n",
      "--finished 5000 sentences (word/sec=3789.04)\n",
      "--finished 10000 sentences (word/sec=3596.53)\n",
      "--finished 15000 sentences (word/sec=3574.84)\n",
      "--finished 20000 sentences (word/sec=3546.65)\n",
      "--finished 25000 sentences (word/sec=3528.88)\n",
      "--finished 30000 sentences (word/sec=3562.59)\n",
      "--finished 35000 sentences (word/sec=3581.23)\n",
      "--finished 40000 sentences (word/sec=3577.42)\n",
      "iter 1: train loss/word=5.7680, ppl=319.8887 (word/sec=3569.30)\n",
      "iter 1: dev loss/word=5.7843, ppl=325.1394 (word/sec=20103.14)\n",
      "teddy <unk>\n",
      "credit yesterday he is that two books consumer facility at numerous national responded from creditors and more than N of general europe and growing <unk> N <unk> shock joined concentrated the u.s. and political of both agencies\n",
      "the company testimony to submit the form a <unk> credit-card consulting firm publicly showing inflation in new york will act at in an interview this north currently resigned\n",
      "we will have that <unk> a battle\n",
      "this victory in the columbia ones who <unk> she steel laboratories\n",
      "--finished 5000 sentences (word/sec=3319.21)\n",
      "--finished 10000 sentences (word/sec=3360.50)\n",
      "--finished 15000 sentences (word/sec=3426.68)\n",
      "--finished 20000 sentences (word/sec=3438.75)\n",
      "--finished 25000 sentences (word/sec=3366.04)\n",
      "--finished 30000 sentences (word/sec=3339.46)\n",
      "--finished 35000 sentences (word/sec=3353.17)\n",
      "--finished 40000 sentences (word/sec=3302.59)\n",
      "iter 2: train loss/word=5.6104, ppl=273.2625 (word/sec=3293.58)\n",
      "iter 2: dev loss/word=5.7507, ppl=314.4264 (word/sec=16982.79)\n",
      "the new york 's scientist free workers and as significant growth on eastern news force optimistic\n",
      "a <unk> <unk> & <unk> inc. in september from N miles would grow credit\n",
      "that has a senior trades by it\n",
      "among other machines currently\n",
      "moreover the soviet union says he who call pension\n",
      "--finished 5000 sentences (word/sec=3109.25)\n",
      "--finished 10000 sentences (word/sec=3071.09)\n",
      "--finished 15000 sentences (word/sec=3042.19)\n",
      "--finished 20000 sentences (word/sec=3055.29)\n",
      "--finished 25000 sentences (word/sec=3051.86)\n",
      "--finished 30000 sentences (word/sec=3082.95)\n",
      "--finished 35000 sentences (word/sec=3050.35)\n",
      "--finished 40000 sentences (word/sec=3018.72)\n",
      "iter 3: train loss/word=5.5213, ppl=249.9501 (word/sec=3017.96)\n",
      "iter 3: dev loss/word=5.7213, ppl=305.2910 (word/sec=13704.35)\n",
      "all but three three deep\n",
      "while a year earlier\n",
      "the acquisition of their concerned about help japanese dollars partly for another city university a week follows his nearly black initiative to the with quarter of june N to N N of american city financing nations in their cost businesses to community wide\n",
      "only N N\n",
      "last week\n",
      "--finished 5000 sentences (word/sec=2694.77)\n",
      "--finished 10000 sentences (word/sec=2480.87)\n",
      "--finished 15000 sentences (word/sec=2576.22)\n",
      "--finished 20000 sentences (word/sec=2665.04)\n",
      "--finished 25000 sentences (word/sec=2575.93)\n",
      "--finished 30000 sentences (word/sec=2566.05)\n",
      "--finished 35000 sentences (word/sec=2531.88)\n",
      "--finished 40000 sentences (word/sec=2525.22)\n",
      "iter 4: train loss/word=5.4554, ppl=234.0113 (word/sec=2496.84)\n",
      "iter 4: dev loss/word=5.7153, ppl=303.4611 (word/sec=8379.15)\n",
      "maxicare 's expected N to $ N million\n",
      "the market <unk> to be the lead N the company 's own deal would be the affected <unk> before first agreement down such money for power\n",
      "the 's settlement of after believed to go up officials said\n",
      "departments\n",
      "the among other microsoft and got all hope to ' <unk> partner in the upside\n"
     ]
    }
   ],
   "source": [
    "\n",
    "last_dev = 1e20\n",
    "best_dev = 1e20\n",
    "\n",
    "for ITER in range(5):\n",
    "  # Perform training\n",
    "    random.shuffle(train)\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for sent_id, sent in enumerate(train):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        train_loss += my_loss.data[0]\n",
    "        train_words += len(sent)\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "        if (sent_id+1) % 5000 == 0:\n",
    "            print(\"--finished %r sentences (word/sec=%.2f)\" % (sent_id+1, train_words/(time.time()-start)))\n",
    "    print(\"iter %r: train loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), train_words/(time.time()-start)))\n",
    "\n",
    "    # Evaluate on dev set\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for sent_id, sent in enumerate(dev):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        dev_loss += my_loss.data[0]\n",
    "        dev_words += len(sent)\n",
    "\n",
    "    # Keep track of the development accuracy and reduce the learning rate if it got worse\n",
    "    if last_dev < dev_loss:\n",
    "        optimizer.learning_rate /= 2\n",
    "    last_dev = dev_loss\n",
    "\n",
    "    # Keep track of the best development accuracy, and save the model only if it's the best one\n",
    "    if best_dev > dev_loss:\n",
    "        torch.save(model, \"model.pt\")\n",
    "        best_dev = dev_loss\n",
    "\n",
    "    # Save the model\n",
    "    print(\"iter %r: dev loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), dev_words/(time.time()-start)))\n",
    "\n",
    "    # Generate a few sentences\n",
    "    for _ in range(5):\n",
    "        sent = generate_sent()\n",
    "        print(\" \".join([i2w[x] for x in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
